# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #5 выполнил(а):
- Ильясова Лейсан Рамилевна
- РИ220936
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | # |  |
| Задание 2 | # |  |
| Задание 3 | # |  |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.
- ✨Magic ✨

## Цель работы
Познакомиться с программными средствами для создания системы машинного обучения и ее интеграции в Unity.

## Задание 1
### Найдите внутри C# скрипта “коэффициент корреляции ” и сделать выводы о том, как он влияет на обучение модели.

Жертва была найдена в методе OnActionReceived:

```py
float distanceToTarget = Vector3.Distance(this.transform.localPosition, Target.localPosition);

        if (distanceToTarget < 1.42f)
        {
            SetReward(1.0f);
            EndEpisode();
        }
```

Коэффициент корреляции определяет необходимое минимальное расстояние между целью и объектом для успешного выполнения задачи. Чем он меньше, тем точнее будет обучен агент (но тем больше времени займет время его обучения).

## Задание 2
### Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров.

Содержимое rollerball_config.yaml:

```py
behaviors:
  RollerBall:
    trainer_type: ppo
    hyperparameters:
      batch_size: 10
      buffer_size: 100
      learning_rate: 3.0e-4
      beta: 5.0e-4
      epsilon: 0.2
      lambd: 0.99
      num_epoch: 3
      learning_rate_schedule: linear
    network_settings:
      normalize: false
      hidden_units: 128
      num_layers: 2
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    max_steps: 500000
    time_horizon: 64
    summary_freq: 10000
```

num_epoch. 
Количество эпох обучения. Влияет на точность и время обучения. Чем их больше, тем точнее будет обучен агент, но это займет больше времени.

time_horizon.
Rоличество шагов, которое должен сделать агент, прежде чем он получит награду и будет добавлен в буфер опыта. Если эпизоды обучения слишком велики, то агент будет выполнять много лишних действий, но если число будет слишком маленьким, то обучение может стать невозможным.

max_steps.
Максимальное количество выполняемых действий во время обучении до достижения цели.

hyperparameters: epsilon.
Влияет на расхождения действий агента в прошлой и нынешней итерации. Чем меньше значение, тем меньше будет расхождение, соответственно обучение будет стабильнее, но медленнее. Чем больше, тем нестабильнее, но "обширнее?".


## Задание 3
### Приведите примеры, для каких игровых задачи и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения? 

Первый агент подойдет для NPC врагов, приближающихся и атакующих игрока. Или наоборот, союзных NPC, следующих за игроком (какие-нибудь простые миссии сопровождения или просто переходы с точки A в точку B).
Второго агента тоже можно использовать для NPC, населяющих какую-либо локацию для имитации оживленности.

Если ручное решение слишком трудоемкое и трудночитаемое. Или если от ML-агента требуется адаптация к действиям игрока или среды.

## Выводы

Ознакомились с созданием ML-агентов, их правильной интеграцией в Unity и обучением. Были изучены yaml файлы и их параметры, влияющие на обучение, а также определены ситуации, требующие использования ML-агентов.

| Plugin | README |
| ------ | ------ |
| GitHub | [plugins/github/README.md][PlGh] |
| Google Drive | [plugins/googledrive/README.md][PlGd] |

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
